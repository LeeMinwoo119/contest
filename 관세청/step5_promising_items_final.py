import numpy as np
import pandas as pd
import torch
from stable_baselines3 import PPO
from step3_environment import PromisingItemSelectionEnv
from step4_ppo_agent import create_training_env
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PromisingItemsExtractor:
    """
    ìœ ë§í’ˆëª© ìƒìœ„ 5ê°œ ì¶”ì¶œê¸°
    """
    
    def __init__(self, model_path="ppo_promising_items_model"):
        """
        ì¶”ì¶œê¸° ì´ˆê¸°í™”
        """
        self.env = create_training_env()
        
        # ëª¨ë¸ ë¡œë“œ (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)
        try:
            self.model = PPO.load(model_path)
            logger.info(f"ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
        
        # í’ˆëª© ì •ë³´ ë¡œë“œ
        self.item_data = pd.read_csv('normalized_indicators.csv')
        
        # ë³´ìƒ ê°€ì¤‘ì¹˜ (í™˜ê²½ê³¼ ë™ì¼)
        self.reward_weights = np.array([0.3, 0.3, 0.2, 0.15, 0.15])
        
        logger.info(f"ì¶”ì¶œê¸° ì´ˆê¸°í™” ì™„ë£Œ: {len(self.item_data)}ê°œ í’ˆëª©")
    
    def method1_direct_reward_calculation(self, top_k=5):
        """
        ë°©ë²• 1: ì§ì ‘ ë³´ìƒ ê³„ì‚°ìœ¼ë¡œ ìƒìœ„ Kê°œ ì¶”ì¶œ (ìˆ˜í•™ì  ìµœì í•´)
        """
        print(f"\n=== ë°©ë²• 1: ì§ì ‘ ë³´ìƒ ê³„ì‚° (ìˆ˜í•™ì  ìµœì í•´) ===")
        
        # ëª¨ë“  í’ˆëª©ì˜ ë³´ìƒ ê³„ì‚°
        all_rewards = []
        for idx, row in self.item_data.iterrows():
            # ì •ê·œí™”ëœ ì§€í‘œ ì¶”ì¶œ
            indicators = np.array([
                row['CAGR'], row['RCA_ë³€í™”ìœ¨'], row['ì ìœ ìœ¨_ë³€í™”ìœ¨'], 
                row['TSC'], row['ìˆ˜ì¶œë‹¨ê°€_ë³€í™”ìœ¨']
            ])
            
            # ë³´ìƒ ê³„ì‚°
            reward = np.dot(indicators, self.reward_weights)
            all_rewards.append({
                'hs_code': row['HSì½”ë“œ'],
                'item_name': row['í’ˆëª©ëª…'],
                'reward': reward,
                'cagr': row['CAGR'],
                'rca_change': row['RCA_ë³€í™”ìœ¨'],
                'share_change': row['ì ìœ ìœ¨_ë³€í™”ìœ¨'],
                'tsc': row['TSC'],
                'price_change': row['ìˆ˜ì¶œë‹¨ê°€_ë³€í™”ìœ¨']
            })
        
        # ë³´ìƒ ê¸°ì¤€ ì •ë ¬
        all_rewards.sort(key=lambda x: x['reward'], reverse=True)
        
        # ìƒìœ„ Kê°œ ì„ íƒ
        top_items = all_rewards[:top_k]
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"ìƒìœ„ {top_k}ê°œ í’ˆëª©:")
        for i, item in enumerate(top_items):
            print(f"{i+1}. HS{item['hs_code']:02d}: {item['item_name'][:50]}...")
            print(f"   ë³´ìƒ: {item['reward']:.4f}")
            print(f"   ì§€í‘œ: CAGR={item['cagr']:.3f}, RCAë³€í™”={item['rca_change']:.3f}, " +
                  f"ì ìœ ìœ¨ë³€í™”={item['share_change']:.3f}, TSC={item['tsc']:.3f}, " +
                  f"ë‹¨ê°€ë³€í™”={item['price_change']:.3f}")
            print()
        
        return top_items
    
    def method2_policy_probability_single_step(self, top_k=5):
        """
        ë°©ë²• 2: í•™ìŠµëœ ì •ì±…ì˜ ì´ˆê¸° í™•ë¥  ë¶„í¬ë¡œ ìƒìœ„ Kê°œ ì¶”ì¶œ
        """
        print(f"\n=== ë°©ë²• 2: í•™ìŠµëœ ì •ì±… í™•ë¥  ë¶„í¬ ===")
        
        # ì´ˆê¸° ìƒíƒœì—ì„œ ì•¡ì…˜ í™•ë¥  ê³„ì‚°
        obs, info = self.env.reset()
        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
        
        # ì •ì±… ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ ì•¡ì…˜ í™•ë¥  ê³„ì‚°
        with torch.no_grad():
            action_probs = self.model.policy.get_distribution(obs_tensor).distribution.probs
            action_probs = action_probs.cpu().numpy().flatten()
        
        # í™•ë¥  ê¸°ì¤€ ì •ë ¬
        prob_rankings = []
        for idx, prob in enumerate(action_probs):
            item_row = self.item_data.iloc[idx]
            prob_rankings.append({
                'hs_code': item_row['HSì½”ë“œ'],
                'item_name': item_row['í’ˆëª©ëª…'],
                'probability': prob,
                'reward': self.env._calculate_reward(idx),
                'cagr': item_row['CAGR'],
                'rca_change': item_row['RCA_ë³€í™”ìœ¨'],
                'share_change': item_row['ì ìœ ìœ¨_ë³€í™”ìœ¨'],
                'tsc': item_row['TSC'],
                'price_change': item_row['ìˆ˜ì¶œë‹¨ê°€_ë³€í™”ìœ¨']
            })
        
        # í™•ë¥  ê¸°ì¤€ ì •ë ¬
        prob_rankings.sort(key=lambda x: x['probability'], reverse=True)
        
        # ìƒìœ„ Kê°œ ì„ íƒ
        top_items = prob_rankings[:top_k]
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"ìƒìœ„ {top_k}ê°œ í’ˆëª©:")
        for i, item in enumerate(top_items):
            print(f"{i+1}. HS{item['hs_code']:02d}: {item['item_name'][:50]}...")
            print(f"   í™•ë¥ : {item['probability']:.4f}, ë³´ìƒ: {item['reward']:.4f}")
            print(f"   ì§€í‘œ: CAGR={item['cagr']:.3f}, RCAë³€í™”={item['rca_change']:.3f}, " +
                  f"ì ìœ ìœ¨ë³€í™”={item['share_change']:.3f}, TSC={item['tsc']:.3f}, " +
                  f"ë‹¨ê°€ë³€í™”={item['price_change']:.3f}")
            print()
        
        return top_items
    
    def method3_guided_selection(self, top_k=5):
        """
        ë°©ë²• 3: ê°€ì´ë“œëœ ì„ íƒ - ë§¤ë²ˆ ì„ íƒ ê°€ëŠ¥í•œ í’ˆëª© ì¤‘ ìµœì„  ì„ íƒ
        """
        print(f"\n=== ë°©ë²• 3: ê°€ì´ë“œëœ ì„ íƒ (ì¤‘ë³µ ì œê±°) ===")
        
        obs, info = self.env.reset()
        selected_items = []
        
        for step in range(top_k):
            # í˜„ì¬ ì„ íƒ ê°€ëŠ¥í•œ í’ˆëª©ë“¤
            available_items = self.env.available_items
            
            if not available_items:
                print(f"ë” ì´ìƒ ì„ íƒ ê°€ëŠ¥í•œ í’ˆëª©ì´ ì—†ìŠµë‹ˆë‹¤. (í˜„ì¬ {len(selected_items)}ê°œ ì„ íƒë¨)")
                break
            
            # ì„ íƒ ê°€ëŠ¥í•œ í’ˆëª©ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í’ˆëª© ì„ íƒ
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
            with torch.no_grad():
                action_probs = self.model.policy.get_distribution(obs_tensor).distribution.probs
                action_probs = action_probs.cpu().numpy().flatten()
            
            # ì„ íƒ ê°€ëŠ¥í•œ í’ˆëª©ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥  ì°¾ê¸°
            best_action = None
            best_prob = -1
            
            for action in available_items:
                if action_probs[action] > best_prob:
                    best_prob = action_probs[action]
                    best_action = action
            
            # ì„ íƒ ì‹¤í–‰
            obs, reward, terminated, truncated, info = self.env.step(best_action)
            
            # í’ˆëª© ì •ë³´ ì €ì¥
            item_row = self.item_data.iloc[best_action]
            selected_items.append({
                'hs_code': item_row['HSì½”ë“œ'],
                'item_name': item_row['í’ˆëª©ëª…'],
                'probability': best_prob,
                'reward': reward,
                'cagr': item_row['CAGR'],
                'rca_change': item_row['RCA_ë³€í™”ìœ¨'],
                'share_change': item_row['ì ìœ ìœ¨_ë³€í™”ìœ¨'],
                'tsc': item_row['TSC'],
                'price_change': item_row['ìˆ˜ì¶œë‹¨ê°€_ë³€í™”ìœ¨']
            })
            
            print(f"{step+1}. HS{item_row['HSì½”ë“œ']:02d}: {item_row['í’ˆëª©ëª…'][:50]}...")
            print(f"   í™•ë¥ : {best_prob:.4f}, ë³´ìƒ: {reward:.4f}")
            print(f"   ì§€í‘œ: CAGR={item_row['CAGR']:.3f}, RCAë³€í™”={item_row['RCA_ë³€í™”ìœ¨']:.3f}, " +
                  f"ì ìœ ìœ¨ë³€í™”={item_row['ì ìœ ìœ¨_ë³€í™”ìœ¨']:.3f}, TSC={item_row['TSC']:.3f}, " +
                  f"ë‹¨ê°€ë³€í™”={item_row['ìˆ˜ì¶œë‹¨ê°€_ë³€í™”ìœ¨']:.3f}")
            print()
            
            if terminated or truncated:
                break
        
        return selected_items
    
    def method4_multiple_runs_consensus(self, top_k=5, n_runs=20):
        """
        ë°©ë²• 4: ë‹¤ì¤‘ ì‹¤í–‰ í•©ì˜ - ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•´ì„œ ê°€ì¥ ë§ì´ ì„ íƒëœ í’ˆëª©ë“¤
        """
        print(f"\n=== ë°©ë²• 4: ë‹¤ì¤‘ ì‹¤í–‰ í•©ì˜ ({n_runs}íšŒ) ===")
        
        # í’ˆëª©ë³„ ì„ íƒ íšŸìˆ˜ ì¶”ì 
        selection_count = {}
        selection_rewards = {}
        
        for run in range(n_runs):
            obs, info = self.env.reset()
            run_selections = []
            
            for step in range(top_k):
                available_items = self.env.available_items
                
                if not available_items:
                    break
                
                # í™•ë¥ ì  ì„ íƒ (ë‹¤ì–‘ì„± í™•ë³´)
                action, _ = self.model.predict(obs, deterministic=False)
                action = int(action)  # numpy ë°°ì—´ì„ ì •ìˆ˜ë¡œ ë³€í™˜
                
                # ì„ íƒ ê°€ëŠ¥í•œ í’ˆëª©ì¸ì§€ í™•ì¸
                if action in available_items:
                    obs, reward, terminated, truncated, info = self.env.step(action)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    if action not in selection_count:
                        selection_count[action] = 0
                        selection_rewards[action] = []
                    
                    selection_count[action] += 1
                    selection_rewards[action].append(reward)
                    
                    run_selections.append(action)
                    
                    if terminated or truncated:
                        break
                else:
                    # ì„ íƒ ë¶ˆê°€ëŠ¥í•œ í’ˆëª©ì´ë©´ ê°€ì¥ ë†’ì€ í™•ë¥  í’ˆëª© ì„ íƒ
                    obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
                    with torch.no_grad():
                        action_probs = self.model.policy.get_distribution(obs_tensor).distribution.probs
                        action_probs = action_probs.cpu().numpy().flatten()
                    
                    # ì„ íƒ ê°€ëŠ¥í•œ í’ˆëª© ì¤‘ ìµœê³  í™•ë¥  ì„ íƒ
                    best_action = available_items[np.argmax([action_probs[a] for a in available_items])]
                    obs, reward, terminated, truncated, info = self.env.step(best_action)
                    
                    if best_action not in selection_count:
                        selection_count[best_action] = 0
                        selection_rewards[best_action] = []
                    
                    selection_count[best_action] += 1
                    selection_rewards[best_action].append(reward)
                    
                    if terminated or truncated:
                        break
        
        # ì„ íƒ íšŸìˆ˜ ê¸°ì¤€ ì •ë ¬
        consensus_items = []
        for item_idx, count in selection_count.items():
            item_row = self.item_data.iloc[item_idx]
            avg_reward = np.mean(selection_rewards[item_idx])
            
            consensus_items.append({
                'hs_code': item_row['HSì½”ë“œ'],
                'item_name': item_row['í’ˆëª©ëª…'],
                'selection_count': count,
                'selection_frequency': count / n_runs,
                'avg_reward': avg_reward,
                'cagr': item_row['CAGR'],
                'rca_change': item_row['RCA_ë³€í™”ìœ¨'],
                'share_change': item_row['ì ìœ ìœ¨_ë³€í™”ìœ¨'],
                'tsc': item_row['TSC'],
                'price_change': item_row['ìˆ˜ì¶œë‹¨ê°€_ë³€í™”ìœ¨']
            })
        
        # ì„ íƒ íšŸìˆ˜ ê¸°ì¤€ ì •ë ¬
        consensus_items.sort(key=lambda x: x['selection_count'], reverse=True)
        
        # ìƒìœ„ Kê°œ ì„ íƒ
        top_items = consensus_items[:top_k]
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"ìƒìœ„ {top_k}ê°œ í’ˆëª©:")
        for i, item in enumerate(top_items):
            print(f"{i+1}. HS{item['hs_code']:02d}: {item['item_name'][:50]}...")
            print(f"   ì„ íƒ íšŸìˆ˜: {item['selection_count']}/{n_runs} (ë¹ˆë„: {item['selection_frequency']:.3f})")
            print(f"   í‰ê·  ë³´ìƒ: {item['avg_reward']:.4f}")
            print(f"   ì§€í‘œ: CAGR={item['cagr']:.3f}, RCAë³€í™”={item['rca_change']:.3f}, " +
                  f"ì ìœ ìœ¨ë³€í™”={item['share_change']:.3f}, TSC={item['tsc']:.3f}, " +
                  f"ë‹¨ê°€ë³€í™”={item['price_change']:.3f}")
            print()
        
        return top_items
    
    def extract_final_promising_items(self, top_k=5):
        """
        ìµœì¢… ìœ ë§í’ˆëª© ì¶”ì¶œ - 4ê°€ì§€ ë°©ë²• ì¢…í•©
        """
        print("="*80)
        print("ğŸ¯ ìµœì¢… ìœ ë§í’ˆëª© ì¶”ì¶œ - 4ê°€ì§€ ë°©ë²• ì¢…í•© ë¶„ì„")
        print("="*80)
        
        # 4ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì¶”ì¶œ
        method1_results = self.method1_direct_reward_calculation(top_k)
        method2_results = self.method2_policy_probability_single_step(top_k)
        method3_results = self.method3_guided_selection(top_k)
        method4_results = self.method4_multiple_runs_consensus(top_k, n_runs=20)
        
        # ì¢…í•© ë¶„ì„
        print(f"\nğŸ” ì¢…í•© ë¶„ì„ ë° ìµœì¢… ì¶”ì²œ:")
        print("=" * 80)
        
        # ê° ë°©ë²•ì—ì„œ ë‚˜ì˜¨ í’ˆëª©ë“¤ ìˆ˜ì§‘
        all_items = {}
        methods = [
            ("ìˆ˜í•™ì  ìµœì í•´", method1_results),
            ("ì •ì±… í™•ë¥ ", method2_results),
            ("ê°€ì´ë“œëœ ì„ íƒ", method3_results),
            ("ë‹¤ì¤‘ ì‹¤í–‰ í•©ì˜", method4_results)
        ]
        
        for method_name, results in methods:
            for i, item in enumerate(results):
                hs_code = item['hs_code']
                if hs_code not in all_items:
                    all_items[hs_code] = {
                        'hs_code': hs_code,
                        'item_name': item['item_name'],
                        'methods': [method_name],
                        'ranks': [i + 1],
                        'avg_rank': i + 1,
                        'appearance_count': 1,
                        'total_score': top_k - i,  # ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜
                        'best_reward': item.get('reward', item.get('avg_reward', 0))
                    }
                else:
                    all_items[hs_code]['methods'].append(method_name)
                    all_items[hs_code]['ranks'].append(i + 1)
                    all_items[hs_code]['avg_rank'] = np.mean(all_items[hs_code]['ranks'])
                    all_items[hs_code]['appearance_count'] += 1
                    all_items[hs_code]['total_score'] += (top_k - i)
        
        # ì¢…í•© ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ (ì¶œí˜„ íšŸìˆ˜ Ã— ì´ ì ìˆ˜)
        final_ranking = sorted(all_items.values(), 
                              key=lambda x: (x['appearance_count'] * x['total_score']), 
                              reverse=True)
        
        # ìµœì¢… ìƒìœ„ 5ê°œ ì¶”ì²œ
        final_top5 = final_ranking[:top_k]
        
        print(f"ğŸ“Š ìµœì¢… ì¶”ì²œ ìœ ë§í’ˆëª© ìƒìœ„ {top_k}ê°œ:")
        print("-" * 80)
        
        for i, item in enumerate(final_top5):
            print(f"{i+1}. HS{item['hs_code']:02d}: {item['item_name']}")
            print(f"   ì¶œí˜„ íšŸìˆ˜: {item['appearance_count']}/4 ë°©ë²•")
            print(f"   ì¶œí˜„ ë°©ë²•: {', '.join(item['methods'])}")
            print(f"   í‰ê·  ìˆœìœ„: {item['avg_rank']:.1f}")
            print(f"   ì¢…í•© ì ìˆ˜: {item['total_score']}")
            print(f"   ìµœê³  ë³´ìƒ: {item['best_reward']:.4f}")
            print()
        
        return {
            'method1': method1_results,
            'method2': method2_results,  
            'method3': method3_results,
            'method4': method4_results,
            'final_top5': final_top5
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ 5ë‹¨ê³„: ìœ ë§í’ˆëª© ìƒìœ„ 5ê°œ ìµœì¢… ì¶”ì¶œ")
    print("="*80)
    
    try:
        # ì¶”ì¶œê¸° ì´ˆê¸°í™”
        extractor = PromisingItemsExtractor()
        
        # ìµœì¢… ìœ ë§í’ˆëª© ì¶”ì¶œ
        results = extractor.extract_final_promising_items(top_k=5)
        
        print(f"\nâœ… ìœ ë§í’ˆëª© ì¶”ì¶œ ì™„ë£Œ!")
        print(f"ğŸ† ìµœì¢… ì¶”ì²œ ìœ ë§í’ˆëª©:")
        print("=" * 50)
        
        for item in results['final_top5']:
            print(f"â€¢ HS{item['hs_code']:02d}: {item['item_name']}")
        
        print(f"\nğŸ‰ í”„ë¡œì íŠ¸ ì™„ë£Œ!")
        print(f"ê°•í™”í•™ìŠµ ê¸°ë°˜ ìœ ë§í’ˆëª© ì„ ë³„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except Exception as e:
        logger.error(f"ìœ ë§í’ˆëª© ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

if __name__ == "__main__":
    main() 