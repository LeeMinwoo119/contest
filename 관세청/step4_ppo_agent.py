import numpy as np
import pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.logger import configure
from step3_environment import PromisingItemSelectionEnv
import logging
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TrainingCallback(BaseCallback):
    """
    í•™ìŠµ ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì½œë°±
    """
    def __init__(self, verbose=0):
        super(TrainingCallback, self).__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []
        
    def _on_step(self) -> bool:
        # ì—í”¼ì†Œë“œ ì™„ë£Œ ì‹œ ë³´ìƒ ê¸°ë¡
        if len(self.locals['dones']) > 0 and self.locals['dones'][0]:
            if len(self.locals['infos']) > 0:
                episode_reward = self.locals['infos'][0].get('episode', {}).get('r', 0)
                episode_length = self.locals['infos'][0].get('episode', {}).get('l', 0)
                
                if episode_reward != 0:
                    self.episode_rewards.append(episode_reward)
                    self.episode_lengths.append(episode_length)
                    
                    if len(self.episode_rewards) % 10 == 0:
                        recent_avg = np.mean(self.episode_rewards[-10:])
                        logger.info(f"ì—í”¼ì†Œë“œ {len(self.episode_rewards)}: ìµœê·¼ 10íšŒ í‰ê·  ë³´ìƒ = {recent_avg:.4f}")
        
        return True
    
    def get_statistics(self):
        """í•™ìŠµ í†µê³„ ë°˜í™˜"""
        return {
            'episode_rewards': self.episode_rewards,
            'episode_lengths': self.episode_lengths,
            'avg_reward': np.mean(self.episode_rewards) if self.episode_rewards else 0,
            'max_reward': np.max(self.episode_rewards) if self.episode_rewards else 0,
            'min_reward': np.min(self.episode_rewards) if self.episode_rewards else 0
        }

def create_training_env():
    """
    í•™ìŠµìš© í™˜ê²½ ìƒì„±
    """
    return PromisingItemSelectionEnv(
        max_selections=20,  # ì‹¤ì œ ì—í”¼ì†Œë“œ ê¸¸ì´
        reward_weights=[0.3, 0.3, 0.2, 0.15, 0.15],
        allow_reselection=False
    )

def train_ppo_agent(total_timesteps=50000, n_envs=4):
    """
    PPO ì—ì´ì „íŠ¸ í•™ìŠµ
    
    Args:
        total_timesteps: ì´ í•™ìŠµ ìŠ¤í… ìˆ˜
        n_envs: ë³‘ë ¬ í™˜ê²½ ìˆ˜
    """
    print("=== 4ë‹¨ê³„: PPO ì—ì´ì „íŠ¸ êµ¬í˜„ (Stable Baselines3) ===")
    print("=" * 60)
    
    # ë²¡í„°í™”ëœ í™˜ê²½ ìƒì„±
    print("1. í•™ìŠµ í™˜ê²½ ìƒì„±...")
    env = make_vec_env(create_training_env, n_envs=n_envs)
    
    # PPO ëª¨ë¸ ìƒì„±
    print("2. PPO ëª¨ë¸ ì´ˆê¸°í™”...")
    model = PPO(
        policy="MlpPolicy",
        env=env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        clip_range_vf=None,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        use_sde=False,
        sde_sample_freq=-1,
        target_kl=None,
        tensorboard_log="./ppo_tensorboard/",
        policy_kwargs=dict(
            net_arch=[128, 128]  # ì€ë‹‰ì¸µ êµ¬ì¡°
        ),
        verbose=1,
        seed=42
    )
    
    print(f"   - ì •ì±… ë„¤íŠ¸ì›Œí¬: MlpPolicy")
    print(f"   - í•™ìŠµë¥ : {model.learning_rate}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {model.batch_size}")
    print(f"   - ì—í¬í¬: {model.n_epochs}")
    print(f"   - í´ë¦¬í•‘ ë²”ìœ„: {model.clip_range}")
    print(f"   - ë³‘ë ¬ í™˜ê²½ ìˆ˜: {n_envs}")
    
    # ì½œë°± ì„¤ì •
    callback = TrainingCallback(verbose=1)
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = "./ppo_logs/"
    os.makedirs(log_dir, exist_ok=True)
    
    # í•™ìŠµ ì‹¤í–‰
    print(f"\n3. PPO ì—ì´ì „íŠ¸ í•™ìŠµ ì‹œì‘...")
    print(f"   - ì´ í•™ìŠµ ìŠ¤í…: {total_timesteps:,}")
    print(f"   - ì˜ˆìƒ ì—í”¼ì†Œë“œ ìˆ˜: {total_timesteps // 20:,}")
    
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback,
        log_interval=10,
        tb_log_name="PPO_PromisingItems"
    )
    
    # ëª¨ë¸ ì €ì¥
    model_path = "ppo_promising_items_model"
    model.save(model_path)
    print(f"\n4. í•™ìŠµëœ ëª¨ë¸ ì €ì¥: {model_path}")
    
    # í•™ìŠµ í†µê³„
    stats = callback.get_statistics()
    print(f"\n5. í•™ìŠµ í†µê³„:")
    print(f"   - ì´ ì—í”¼ì†Œë“œ: {len(stats['episode_rewards'])}")
    print(f"   - í‰ê·  ë³´ìƒ: {stats['avg_reward']:.4f}")
    print(f"   - ìµœëŒ€ ë³´ìƒ: {stats['max_reward']:.4f}")
    print(f"   - ìµœì†Œ ë³´ìƒ: {stats['min_reward']:.4f}")
    
    return model, stats

def test_trained_model(model_path="ppo_promising_items_model", n_test_episodes=5):
    """
    í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    """
    print(f"\n=== í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    print("=" * 40)
    
    # í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„±
    env = create_training_env()
    
    # ëª¨ë¸ ë¡œë“œ
    model = PPO.load(model_path)
    print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
    
    # í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ì‹¤í–‰
    test_rewards = []
    test_selected_items = []
    
    for episode in range(n_test_episodes):
        obs, info = env.reset()  # Gymnasium API: tuple ë°˜í™˜
        episode_reward = 0
        selected_items = []
        
        print(f"\ní…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ {episode + 1}:")
        
        while True:
            # ëª¨ë¸ì„ ì‚¬ìš©í•œ ì•¡ì…˜ ì˜ˆì¸¡ (obsë§Œ ì „ë‹¬)
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            episode_reward += reward
            selected_items.append({
                'step': len(selected_items) + 1,
                'hs_code': info['selected_item'],
                'item_name': info['item_name'],
                'reward': reward
            })
            
            if done:
                break
        
        test_rewards.append(episode_reward)
        test_selected_items.append(selected_items)
        
        # ìƒìœ„ 3ê°œ ì„ íƒ í’ˆëª© ì¶œë ¥
        top_items = sorted(selected_items, key=lambda x: x['reward'], reverse=True)[:3]
        print(f"  ì´ ë³´ìƒ: {episode_reward:.4f}")
        print(f"  ìƒìœ„ 3ê°œ ì„ íƒ í’ˆëª©:")
        for i, item in enumerate(top_items):
            print(f"    {i+1}. HS{item['hs_code']:02d}: {item['item_name'][:40]}... (ë³´ìƒ: {item['reward']:.4f})")
    
    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
    print(f"\ní…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    print(f"  - í‰ê·  ë³´ìƒ: {np.mean(test_rewards):.4f}")
    print(f"  - ìµœëŒ€ ë³´ìƒ: {np.max(test_rewards):.4f}")
    print(f"  - ìµœì†Œ ë³´ìƒ: {np.min(test_rewards):.4f}")
    print(f"  - í‘œì¤€í¸ì°¨: {np.std(test_rewards):.4f}")
    
    # ê°€ì¥ ì„±ê³¼ê°€ ì¢‹ì€ ì—í”¼ì†Œë“œì˜ ì „ì²´ ì„ íƒ í’ˆëª© ì¶œë ¥
    best_episode_idx = np.argmax(test_rewards)
    best_episode_items = test_selected_items[best_episode_idx]
    
    print(f"\nìµœê³  ì„±ê³¼ ì—í”¼ì†Œë“œ ({best_episode_idx + 1}) ì „ì²´ ì„ íƒ í’ˆëª©:")
    for item in best_episode_items:
        print(f"  {item['step']:2d}. HS{item['hs_code']:02d}: {item['item_name'][:50]}... (ë³´ìƒ: {item['reward']:.4f})")
    
    return test_rewards, test_selected_items

def print_training_summary(stats):
    """
    í•™ìŠµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    """
    if not stats['episode_rewards']:
        print("ì¶œë ¥í•  í•™ìŠµ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\n=== í•™ìŠµ ê²°ê³¼ ìš”ì•½ ===")
    print(f"ì´ ì—í”¼ì†Œë“œ: {len(stats['episode_rewards'])}")
    print(f"í‰ê·  ë³´ìƒ: {stats['avg_reward']:.4f}")
    print(f"ìµœëŒ€ ë³´ìƒ: {stats['max_reward']:.4f}")
    print(f"ìµœì†Œ ë³´ìƒ: {stats['min_reward']:.4f}")
    
    # ìµœê·¼ 10ê°œ ì—í”¼ì†Œë“œ í‰ê· 
    if len(stats['episode_rewards']) >= 10:
        recent_avg = np.mean(stats['episode_rewards'][-10:])
        print(f"ìµœê·¼ 10ê°œ ì—í”¼ì†Œë“œ í‰ê· : {recent_avg:.4f}")
    
    # í•™ìŠµ ì§„í–‰ ìƒí™© (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì°¨íŠ¸)
    if len(stats['episode_rewards']) > 20:
        print(f"\në³´ìƒ ë³€í™” ì¶”ì´ (ìµœê·¼ 20ê°œ ì—í”¼ì†Œë“œ):")
        recent_rewards = stats['episode_rewards'][-20:]
        for i, reward in enumerate(recent_rewards):
            bar_length = int(reward * 20)  # ë³´ìƒì„ 20ë°° ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            bar = "â–ˆ" * bar_length
            print(f"  {i+1:2d}: {reward:.3f} {bar}")
    
    print(f"\nğŸ’¡ ë” ìì„¸í•œ í•™ìŠµ ëª¨ë‹ˆí„°ë§ì€ í…ì„œë³´ë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:")
    print(f"   tensorboard --logdir=./ppo_tensorboard/")

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("Stable Baselines3ë¥¼ ì‚¬ìš©í•œ PPO ì—ì´ì „íŠ¸ êµ¬í˜„")
    print("=" * 60)
    
    # í•™ìŠµ ì‹¤í–‰
    model, stats = train_ppo_agent(total_timesteps=10000, n_envs=2)  # í…ŒìŠ¤íŠ¸ìš© ì‘ì€ ê°’
    
    # í•™ìŠµ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print_training_summary(stats)
    
    # í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    test_rewards, test_items = test_trained_model()
    
    print(f"\n=== 4ë‹¨ê³„ ì™„ë£Œ ===")
    print(f"PPO ì—ì´ì „íŠ¸ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ë‹¤ìŒ ë‹¨ê³„: 5ë‹¨ê³„ - ëŒ€ê·œëª¨ í•™ìŠµ ë° ìµœì¢… í‰ê°€")

if __name__ == "__main__":
    main() 